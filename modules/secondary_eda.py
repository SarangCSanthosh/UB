import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
from plotly.subplots import make_subplots
from streamlit_plotly_events import plotly_events

# --------------------------
# HELPERS
# --------------------------
@st.cache_data
def load_excel(path_or_file, sheet_name=0):
    return pd.read_excel(path_or_file, sheet_name=sheet_name)

@st.cache_data
def load_normalized_data(path):
    df_norm = pd.read_excel(path)

    # Rename first column to ACTUAL_DATE
    df_norm.rename(columns={df_norm.columns[0]: "ACTUAL_DATE"}, inplace=True)

    # Ensure datetime
    df_norm["ACTUAL_DATE"] = pd.to_datetime(df_norm["ACTUAL_DATE"], errors="coerce")

    # Sum across all numeric columns (excluding ACTUAL_DATE)
    numeric_cols = df_norm.select_dtypes(include=[np.number]).columns
    df_norm["VOLUME"] = df_norm[numeric_cols].sum(axis=1)

    # Extract YearMonth
    df_norm["YearMonth"] = df_norm["ACTUAL_DATE"].dt.to_period("M").astype(str)

    return df_norm[["ACTUAL_DATE", "YearMonth", "VOLUME"]]

def prepare_dates(df, date_col="ACTUAL_DATE"):
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df["Year"] = df[date_col].dt.year
    df["YearMonth"] = df[date_col].dt.to_period("M")
    df["Quarter"] = df[date_col].dt.to_period("Q")
    return df, date_col

@st.cache_data
def load_event_calendar(sheet_id: str):
    """
    Loads the event calendar from a public Google Sheet.
    Input: Google Sheet ID
    Output: DataFrame
    """
    try:
        # Build a direct download link
        
        #download_url = f"https://docs.google.com/spreadsheets/d/1QYN4ZHmB-FpA1wUFlzh5Vp-WtMFPV8jO/export?format=xlsx"
        download_url = f"https://docs.google.com/spreadsheets/d/1PZSyJWB_1iPbARkUOiNOVooF51PjDhxlgGxgCdSCzKk/export?format=xlsx"
        df = pd.read_excel(download_url)

        # Clean and standardize columns
        df.columns = df.columns.str.strip()
        if "Date" in df.columns:
            df["Date"] = pd.to_datetime(df["Date"], errors="coerce")

        # Drop rows without valid dates
        df = df.dropna(subset=["Date"])

        return df

    except Exception as e:
        st.error(f"❌ Could not load event calendar: {e}")
        return pd.DataFrame(columns=["Date", "Day", "Month", "Week Number", "Event / Task", "Remarks"])




# --------------------------
# MAIN APP
# --------------------------
def run():
    st.title("Secondary Dataset Dashboard")

    # --------------------------
    # LOAD DATA
    # --------------------------
    default_path = "https://docs.google.com/spreadsheets/d/1te1MVxSoO3EWwg_9akooxKxIEgI4KDna/export?format=xlsx"
    df = load_excel(default_path)
    SHEET_ID = "1QYN4ZHmB-FpA1wUFlzh5Vp-WtMFPV8jO"
    df_events = load_event_calendar(SHEET_ID)

    VOLUME_COL = "VOLUME"
    OUTLET_COL = "DBF_OUTLET_NAME"

    df, DATE_COL = prepare_dates(df)

    # --------------------------
    # FIXED KPIs
    # --------------------------
    yearly_data = df.groupby("Year").agg(
        Total_Volume=(VOLUME_COL, "sum"),
        Unique_Outlets=(OUTLET_COL, "nunique"),
        Total_Shipments=(DATE_COL, "count"),
    ).sort_index()

    latest_year = yearly_data.index.max()
    prev_year = latest_year - 1 if latest_year - 1 in yearly_data.index else None

    def pct_delta(cur, prev):
        if prev is None or prev == 0:
            return None
        return ((cur - prev) / prev) * 100

    if prev_year:
        kpi_volume = yearly_data.loc[latest_year, "Total_Volume"]
        kpi_outlets = yearly_data.loc[latest_year, "Unique_Outlets"]
        kpi_shipments = yearly_data.loc[latest_year, "Total_Shipments"]

        delta_volume = pct_delta(kpi_volume, yearly_data.loc[prev_year, "Total_Volume"])
        delta_outlets = pct_delta(kpi_outlets, yearly_data.loc[prev_year, "Unique_Outlets"])
        delta_shipments = pct_delta(kpi_shipments, yearly_data.loc[prev_year, "Total_Shipments"])
    else:
        kpi_volume = kpi_outlets = kpi_shipments = 0
        delta_volume = delta_outlets = delta_shipments = None

    def format_delta(val):
        return f"{val:+.0f}%" if val is not None and pd.notna(val) else "N/A"

    # --------------------------
    # SHOW FIXED KPIs
    # --------------------------
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Volume", f"{int(kpi_volume):,}", format_delta(delta_volume))
    col2.metric("Unique Outlets", f"{kpi_outlets}", format_delta(delta_outlets))
    col3.metric("Total Shipments", f"{kpi_shipments}", format_delta(delta_shipments))
    st.caption(f"YoY change: {latest_year} vs {prev_year}")
    st.markdown("---")

    # --------------------------
    # SIDEBAR FILTERS
    # --------------------------
    st.sidebar.header("Filters")

    filter_mode = st.sidebar.radio("Filter by:", ["Year", "Date Range"], horizontal=True)
    df_filtered = df.copy()

    if filter_mode == "Year":
        year_choice = st.sidebar.multiselect(
            "Select Year(s)", options=sorted(df["Year"].dropna().unique()), default=sorted(df["Year"].dropna().unique())
        )
        if year_choice:
            df_filtered = df_filtered[df_filtered["Year"].isin(year_choice)]
    else:
        start_date = st.sidebar.date_input("Start Date", df[DATE_COL].min().date())
        end_date = st.sidebar.date_input("End Date", df[DATE_COL].max().date())
        mask = (df_filtered[DATE_COL].dt.date >= start_date) & (
            df_filtered[DATE_COL].dt.date <= end_date
        )
        df_filtered = df_filtered.loc[mask]

        # --------------------------------------------
    # Apply the same sidebar filters to df_events
    # --------------------------------------------
    df_events_filtered = df_events.copy()
    
    # Ensure Date column is datetime
    df_events_filtered["Date"] = pd.to_datetime(df_events_filtered["Date"], errors="coerce")
    df_events_filtered = df_events_filtered.dropna(subset=["Date"])
    
    if filter_mode == "Year":
        if year_choice:
            df_events_filtered = df_events_filtered[df_events_filtered["Date"].dt.year.isin(year_choice)]
    else:
        mask_events = (
            (df_events_filtered["Date"].dt.date >= start_date)
            & (df_events_filtered["Date"].dt.date <= end_date)
        )
        df_events_filtered = df_events_filtered.loc[mask_events]


    # --------------------------
    # VISUALIZATIONS (tabs)
    # --------------------------
    tab1, tab2, tab3, tab4, tab5, tab6, tab7,tab8 = st.tabs([
        "Shipment Trends",
        "Top Outlets",
        "Depot Analysis",
        "Region Donut",
        "Region Stacked",
        "Special Outlets",
        "Map",
        "Depot-wise YoY Change"
    ])

    # ---- Shipment Trends ----
    with tab1:
        st.markdown("###  Question: Do shipment trends look different by year, quarter, or month?")
        st.subheader("Shipment Trends and Event Calendar")
    
        # --- Chart Selection ---
        chart_type = st.radio(
            "Select Chart Type:",
            ["Shipment Trend", "Event Calendar"],
            horizontal=True
        )
    
        
    
        # ===============================
        # 📊 IF SHIPMENT TREND SELECTED
        # ===============================
        if chart_type == "Shipment Trend":

            # --- Controls (Granularity and View Mode) ---
            granularity = st.radio(
                "Granularity", ["Yearly", "Quarterly", "Monthly"],  # ✅ Removed "Daily"
                horizontal=True, key="trend_granularity"
            )
            view_mode = st.radio(
                "Display Mode", ["Absolute", "Percentage"],
                horizontal=True, key="trend_view"
            )
            # --- Shipment Trend (Filtered Main Data) ---
            if granularity == "Yearly":
                df_filtered["Label"] = df_filtered["Year"].astype(int).astype(str)
            elif granularity == "Quarterly":
                df_filtered["Label"] = df_filtered["Quarter"].astype(str)
            else:  # Monthly granularity
                df_filtered["Label"] = df_filtered["YearMonth"].astype(str)
    
            trend_df = df_filtered.groupby("Label")[VOLUME_COL].sum().reset_index()
    
            if view_mode == "Percentage":
                total_sum = trend_df[VOLUME_COL].sum()
                trend_df["Value"] = (trend_df[VOLUME_COL] / total_sum) * 100
                y_title = "Percentage (%)"
            else:
                trend_df["Value"] = trend_df[VOLUME_COL]
                y_title = "Volume"
    
            # --- Load and Filter Normalised Data ---
            df_normalised = load_normalized_data(
                r"https://docs.google.com/spreadsheets/d/1lg0iIgKx9byQj7d2NZO-k1gKdFuNxxQe/export?format=xlsx"
            )
    
            if filter_mode == "Year":
                if year_choice:
                    df_normalised = df_normalised[df_normalised["ACTUAL_DATE"].dt.year.isin(year_choice)]
            else:
                df_normalised = df_normalised[
                    (df_normalised["ACTUAL_DATE"].dt.date >= start_date)
                    & (df_normalised["ACTUAL_DATE"].dt.date <= end_date)
                ]
    
            df_normalised["Year"] = df_normalised["ACTUAL_DATE"].dt.year
            df_normalised["Quarter"] = df_normalised["ACTUAL_DATE"].dt.to_period("Q").astype(str)
            df_normalised["YearMonth"] = df_normalised["ACTUAL_DATE"].dt.to_period("M").astype(str)
    
            if granularity == "Yearly":
                norm_df = df_normalised.groupby("Year")["VOLUME"].sum().reset_index()
                norm_df["Label"] = norm_df["Year"].astype(str)
            elif granularity == "Quarterly":
                norm_df = df_normalised.groupby("Quarter")["VOLUME"].sum().reset_index()
                norm_df["Label"] = norm_df["Quarter"].astype(str)
            else:
                norm_df = df_normalised.groupby("YearMonth")["VOLUME"].sum().reset_index()
                norm_df["Label"] = norm_df["YearMonth"].astype(str)
    
            if view_mode == "Percentage":
                total_norm = norm_df["VOLUME"].sum()
                norm_df["Normalized_Value"] = (norm_df["VOLUME"] / total_norm) * 100
                norm_y_title = "Normalized Volume (%)"
            else:
                norm_df["Normalized_Value"] = norm_df["VOLUME"]
                norm_y_title = "Normalized Volume"
    
            # --- Load Event Calendar ---
            #EVENT_CSV_URL = "https://docs.google.com/spreadsheets/d/1QYN4ZHmB-FpA1wUFlzh5Vp-WtMFPV8jO/export?format=xlsx"
            EVENT_CSV_URL = "https://docs.google.com/spreadsheets/d/1PZSyJWB_1iPbARkUOiNOVooF51PjDhxlgGxgCdSCzKk/export?format=xlsx"
            df_events = load_event_calendar(EVENT_CSV_URL)
            df_events["Date"] = pd.to_datetime(df_events["Date"], errors="coerce")
    
            if granularity == "Yearly":
                df_events["Label"] = df_events["Date"].dt.year.astype(str)
            elif granularity == "Quarterly":
                df_events["Label"] = df_events["Date"].dt.to_period("Q").astype(str)
            else:
                df_events["Label"] = df_events["Date"].dt.to_period("M").astype(str)
    
            # --- Clean and normalize event names ---
            def clean_event_name(text):
                if pd.isna(text):
                    return None
                text = str(text).strip()
                if not text or text.lower() in ["nan", "none"]:
                    return None

            # Fix common corrupt patterns
                text = text.replace("Against", "")
                text = text.replace("Friendly", "BFC")
                text = text.replace("Footll", "Football")
                text = text.replace("Pro Ka", "Pro Kabbadi")
                text = text.replace("C ", " ")
                text = text.replace("IND World cup", "IND World Cup")
                text = text.replace("RCB Match", "RCB Match")
                text = text.replace("Week end", "Weekend")
                text = text.replace("INDependence", "Independence")
                text = text.replace("Ni8", "Night")
    
                # Remove extra spaces and normalize casing
                text = " ".join(text.split())
                text = text.title().replace("Ipl", "IPL").replace("Bf","BFC")
                return text

            df_events["Event / Task"] = df_events["Event / Task"].apply(clean_event_name)
            
            # --- Aggregate events by Label ---
            def summarize_events(x):
                counts = x.value_counts()
                lines = []
                for event, count in counts.items():
                    if count > 1:
                        lines.append(f"{event} (x{count})")
                    else:
                        lines.append(event)
                return "<br>".join(lines)
    
            events_agg = (
                df_events.groupby("Label", dropna=False)["Event / Task"]
                .apply(summarize_events)
                .reset_index()
            )

            trend_df = trend_df.merge(events_agg, on="Label", how="left")
                
            col1, col2 = st.columns([2, 1])  # left wider for line chart
            
            # ------------------------
            # LEFT: Shipment Trend + Normalized Volume
            # ------------------------
            with col1:
                fig_trend = go.Figure()
                
                # Line: Shipment trend
                fig_trend.add_trace(
                    go.Scatter(
                        x=trend_df["Label"],
                        y=trend_df["Value"],
                        mode="lines+markers",
                        name=f"Shipment Trend ({granularity}, {view_mode})",
                        fill="tozeroy",
                        hovertext=trend_df["Event / Task"],
                        hoverinfo="x+y+text",
                    )
                )
                
                # Line: Normalized volume (secondary y-axis)
                fig_trend.add_trace(
                    go.Scatter(
                        x=norm_df["Label"],
                        y=norm_df["Normalized_Value"],
                        mode="lines+markers",
                        name=f"Normalized {granularity} Volume",
                        line=dict(color="red"),
                        yaxis="y2"
                    )
                )
                
                fig_trend.update_layout(
                    title=f"Shipment Trend vs Normalized Volume ({granularity})",
                    xaxis_title=granularity,
                    yaxis=dict(title=y_title, side="left"),
                    yaxis2=dict(title=norm_y_title, overlaying="y", side="right"),
                    height=500,
                    hovermode="x unified",
                    template="plotly_dark",
                    legend_title="Metrics",
                )
                st.plotly_chart(fig_trend, use_container_width=True)
            
            # ------------------------
            # RIGHT: Bubble Chart of Event Bins
            # ------------------------
            with col2:
                st.subheader("Event Type Distribution")
            
                bin_cols = ["Political", "Festival", "Sports", "Celebrity_Deaths", "Public_Holiday", "Movie", "Weekend"]
            
                # Make a copy and ensure valid dates
                df_events_filtered = df_events.copy()
                df_events_filtered["Date"] = pd.to_datetime(df_events_filtered["Date"], errors="coerce")
                df_events_filtered = df_events_filtered.dropna(subset=["Date"])
            
                # ----------------------------
                # Apply sidebar filters
                # ----------------------------
                if filter_mode == "Year":
                    if year_choice:
                        df_events_filtered = df_events_filtered[df_events_filtered["Date"].dt.year.isin(year_choice)]
                else:
                    mask_events = (
                        (df_events_filtered["Date"].dt.date >= start_date)
                        & (df_events_filtered["Date"].dt.date <= end_date)
                    )
                    df_events_filtered = df_events_filtered.loc[mask_events]
            
                # ----------------------------
                # Bubble Chart
                # ----------------------------
                if df_events_filtered.empty:
                    st.info("ℹ️ No events found for the selected filter.")
                else:
                    bubble_counts = df_events_filtered[bin_cols].sum().reset_index()
                    bubble_counts.columns = ["Event_Type", "Count"]
                    bubble_counts = bubble_counts[bubble_counts["Count"] > 0]
            
                    if bubble_counts.empty:
                        st.info("ℹ️ No events recorded for the selected period.")
                    else:
                        fig_bubble = go.Figure(
                            go.Scatter(
                                x=bubble_counts["Event_Type"],
                                y=[1]*len(bubble_counts),  # same y-level for all bubbles
                                mode="markers+text",
                                marker=dict(
                                    size=bubble_counts["Count"] * 10,  # scale bubble size
                                    color=bubble_counts["Count"],
                                    colorscale="Viridis",
                                    showscale=True,
                                    colorbar=dict(title="Count"),
                                    sizemode="area"
                                ),
                                text=bubble_counts["Count"],
                                textposition="top center",
                                hovertemplate="<b>%{x}</b><br>Count: %{text}<extra></extra>"
                            )
                        )
            
                        fig_bubble.update_layout(
                            title="Event Type Distribution (Filtered)",
                            xaxis_title="Event Type",
                            yaxis=dict(visible=False),  # hide y-axis
                            height=500,
                            template="plotly_dark",
                            margin=dict(t=50)
                        )
            
                        st.plotly_chart(fig_bubble, use_container_width=True)






        # ===============================
        # 📅 IF EVENT CALENDAR SELECTED
        # ===============================
        
        else:
            st.subheader("Event-Based Shipment Visualization")
        
            # --- Load Event Calendar ---
            EVENT_XLSX_URL = "https://docs.google.com/spreadsheets/d/1QYN4ZHmB-FpA1wUFlzh5Vp-WtMFPV8jO/export?format=xlsx"
            df_events = load_event_calendar(EVENT_XLSX_URL)
        
            df_events["Date"] = pd.to_datetime(df_events["Date"], errors="coerce")
            df_events["Year"] = df_events["Date"].dt.year
            df_events["Month"] = df_events["Date"].dt.month
            df_events["MonthName"] = df_events["Date"].dt.strftime("%B")
            df_events["Week"] = df_events["Date"].dt.isocalendar().week
        
            # --- Granularity Selector ---
            granularity = st.radio("Select Granularity", ["Monthly", "Weekly"], horizontal=True)
        
            # --- Shipment Data ---
            df_ship = df.copy()
            df_ship["Date"] = pd.to_datetime(df_ship["ACTUAL_DATE"], errors="coerce")
        
            # =====================================================================
            # ======================== MONTHLY VIEW ===============================
            # =====================================================================
            if granularity == "Monthly":
                st.subheader("Monthly Event Heatmap")
        
                selected_year = st.selectbox("Select Year", sorted(df_events["Year"].dropna().unique()))
                selected_month_name = st.selectbox(
                    "Select Month",
                    sorted(df_events["MonthName"].unique(), key=lambda x: pd.to_datetime(x, format="%B").month)
                )
        
                # --- Filter for selected month ---
                df_selected = df_events[
                    (df_events["Year"] == selected_year) &
                    (df_events["MonthName"] == selected_month_name)
                ].copy()
        
                # --- Merge with shipment data ---
                ship_day = df_ship.groupby(df_ship["Date"].dt.date)[VOLUME_COL].sum().reset_index()
                ship_day.rename(columns={VOLUME_COL: "VOLUME"}, inplace=True)
                ship_day["Date"] = pd.to_datetime(ship_day["Date"], errors="coerce")
        
                df_selected = pd.merge(df_selected, ship_day[["Date", "VOLUME"]], on="Date", how="left")
                df_selected["VOLUME"] = df_selected["VOLUME"].fillna(0)
        
                # --- Prepare calendar grid ---
                month_start = pd.Timestamp(f"{selected_year}-{selected_month_name}-01")
                month_end = (month_start + pd.offsets.MonthEnd(1))
        
                # Create continuous range covering full calendar view (Mon–Sun)
                start_day = month_start - pd.Timedelta(days=month_start.weekday())   # Monday of first week
                end_day = month_end + pd.Timedelta(days=(6 - month_end.weekday()))   # Sunday of last week
                full_range = pd.date_range(start_day, end_day, freq="D")
        
                calendar_df = pd.DataFrame({"Date": full_range})
                calendar_df["Day"] = calendar_df["Date"].dt.day
                calendar_df["DayOfWeek"] = calendar_df["Date"].dt.day_name().str[:3]
                calendar_df["Month"] = calendar_df["Date"].dt.month
                calendar_df["Week"] = ((calendar_df["Date"] - start_day).dt.days // 7) + 1
        
                # Merge shipment volume + tooltip info
                calendar_df["VOLUME"] = calendar_df["Date"].map(df_selected.set_index("Date")["VOLUME"]).fillna(0)
        
                df_selected["Tooltip"] = (
                    "<b>" + df_selected["Date"].dt.strftime("%d %b %Y") + "</b><br>" +
                    "Event: " + df_selected["Event / Task"].fillna("") + "<br>" +
                    "Volume: " + df_selected["VOLUME"].round(0).astype(int).astype(str)
                )
                calendar_df["Tooltip"] = calendar_df["Date"].map(df_selected.set_index("Date")["Tooltip"])
        
                # Hide days outside the selected month
                calendar_df.loc[calendar_df["Month"] != month_start.month, ["VOLUME", "Day", "Tooltip"]] = [None, "", None]
        
                # --- Pivot into heatmap grid ---
                ordered_days = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
                pivot_volume = calendar_df.pivot(index="Week", columns="DayOfWeek", values="VOLUME")[ordered_days]
                text_matrix = calendar_df.pivot(index="Week", columns="DayOfWeek", values="Day")[ordered_days]
                hover_matrix = calendar_df.pivot(index="Week", columns="DayOfWeek", values="Tooltip")[ordered_days]
        
                # --- Plotly Heatmap ---
                fig = go.Figure(
                    data=go.Heatmap(
                        z=pivot_volume.values,
                        x=pivot_volume.columns,
                        y=pivot_volume.index,
                        text=text_matrix.values,
                        texttemplate="%{text}",
                        hovertext=hover_matrix.values,
                        hoverinfo="text",
                        colorscale="RdPu",
                        showscale=True
                    )
                )
        
                fig.update_layout(
                    title=f"{selected_month_name} {selected_year} — Shipment Volume Calendar",
                    xaxis=dict(title="", side="top"),
                    yaxis=dict(title="", autorange="reversed"),
                    width=700,
                    height=450,
                    template="simple_white",
                    margin=dict(l=20, r=20, t=80, b=20),
                    coloraxis_colorbar=dict(title="Volume")
                )
                st.plotly_chart(fig, use_container_width=True)
        
            # =====================================================================
            # ========================= WEEKLY VIEW ===============================
            # =====================================================================
            else:
                st.subheader("Weekly Event Trend")
        
                df_events["Date"] = pd.to_datetime(df_events["Date"], errors="coerce")
                df_events["Year"] = df_events["Date"].dt.year
                df_events["Week"] = df_events["Date"].dt.isocalendar().week
        
                if "Year" not in df_events.columns or df_events["Year"].dropna().empty:
                    st.warning("No valid event dates available to display weekly data.")
                else:
                    selected_year = st.selectbox("Select Year", sorted(df_events["Year"].dropna().unique()))
        
                    # --- Aggregate shipment volume weekly ---
                    df_ship["Year"] = df_ship["Date"].dt.year
                    df_ship["Week"] = df_ship["Date"].dt.isocalendar().week
        
                    weekly_ship = (
                        df_ship[df_ship["Year"] == selected_year]
                        .groupby("Week")[VOLUME_COL]
                        .sum()
                        .reset_index()
                        .rename(columns={VOLUME_COL: "VOLUME"})
                    )
        
                    # --- Compute start & end date for each week ---
                    def get_week_range(year, week):
                        try:
                            start = pd.Timestamp.fromisocalendar(year, int(week), 1)
                            end = pd.Timestamp.fromisocalendar(year, int(week), 7)
                            return start, end
                        except Exception:
                            return pd.NaT, pd.NaT
        
                    weekly_ship["Start_Date"], weekly_ship["End_Date"] = zip(*weekly_ship.apply(
                        lambda x: get_week_range(selected_year, int(x["Week"])), axis=1
                    ))
        
                    weekly_ship["WeekRange"] = (
                        weekly_ship["Start_Date"].dt.strftime("%d %b") + " – " +
                        weekly_ship["End_Date"].dt.strftime("%d %b")
                    )
        
                    # --- Merge with events (combine events per week) ---
                    df_events_week = (
                        df_events[df_events["Year"] == selected_year]
                        .groupby("Week")["Event / Task"]
                        .apply(lambda x: ", ".join(x.dropna().unique()))
                        .reset_index()
                    )
        
                    df_weekly = pd.merge(weekly_ship, df_events_week, on="Week", how="left")
        
                    # --- Tooltip ---
                    df_weekly["Tooltip"] = (
                        "<b>Week " + df_weekly["Week"].astype(str) + "</b><br>" +
                        df_weekly["WeekRange"] + "<br>" +
                        "Volume: " + df_weekly["VOLUME"].astype(int).astype(str) + "<br>" +
                        "Events: " + df_weekly["Event / Task"].fillna("None")
                    )
        
                    # --- Plotly Weekly Bar Chart ---
                    fig_week = go.Figure()
                    fig_week.add_trace(go.Bar(
                        x=df_weekly["Week"],
                        y=df_weekly["VOLUME"],
                        text=df_weekly["VOLUME"],
                        hovertext=df_weekly["Tooltip"],
                        hoverinfo="text",
                        marker_color="mediumvioletred",
                        name="Weekly Volume"
                    ))
        
                    fig_week.update_layout(
                        title=f"{selected_year} — Weekly Shipment Volume & Events",
                        xaxis_title="Week Number",
                        yaxis_title="Volume",
                        template="simple_white",
                        height=450,
                        width=800,
                        margin=dict(l=40, r=40, t=80, b=40)
                    )
        
                    st.plotly_chart(fig_week, use_container_width=True)

        st.markdown("""
### **Insights:**
- There has been a huge dip in the shipment in the month of May-2023. Which is explained by the events that took place during the time period.
- The same dip has been observed in the year 2024 as well but the absolute shipment volume has seen a spike.
- Month wise shipment can be observed through Event Calendar Heat Map. 
""")



    with tab2:
        st.markdown("###  Question: Where is shipment activity the highest among outlets?")
        st.subheader("Top Outlets by Volume")
        
        # --- Controls ---
        view_mode_tab2 = st.radio("Display Mode", ["Absolute", "Percentage"], horizontal=True, key="top_outlets_view_mode")
        top_n = st.slider("Top-N Outlets", 5, 25, 10)
        
        outlet_volume = df_filtered.groupby(OUTLET_COL)[VOLUME_COL].sum().round(0).reset_index()
        top_outlets = outlet_volume.sort_values(by=VOLUME_COL, ascending=False).head(top_n)
        
        if view_mode_tab2 == "Percentage":
            top_outlets["Value"] = (top_outlets[VOLUME_COL] / top_outlets[VOLUME_COL].sum().round(0)) * 100
            value_col = "Value"
            title_suffix = " (%)"
        else:
            top_outlets["Value"] = top_outlets[VOLUME_COL]
            value_col = "Value"
            title_suffix = ""
        
        # Treemap
        fig = px.treemap(top_outlets, path=[OUTLET_COL], values=value_col, 
                         title=f"Top {top_n} Outlets Treemap{title_suffix}",custom_data=[value_col])
        fig.update_traces(
            hovertemplate='<b>%{label}</b><br>Value: %{customdata[0]:,.0f}<extra></extra>'
        )
        st.plotly_chart(fig, use_container_width=True)
        display_df = top_outlets[[OUTLET_COL, value_col]].set_index(OUTLET_COL).round(0)
        st.dataframe(display_df)

        
        st.markdown("""
    ### **Insights:**
    The most striking feature is the overwhelming dominance of a single outlet: MANAGING DIRECTOR MSIL. This outlet takes up easily over 75% to 80% of the total volume visualized in the Top 10 list. The business's volume is heavily reliant on this one outlet. 

    """)


    # ---- Depot Analysis ----
    with tab3:
        st.markdown("###  Question: Which depots are driving the majority of volume?")
        st.subheader("Depot-wise ABC Analysis")
        if "DBF_DEPOT" in df_filtered.columns:
            depot_volume = df_filtered.groupby("DBF_DEPOT")[VOLUME_COL].sum().round(0).reset_index()
            depot_volume = depot_volume.sort_values(by=VOLUME_COL, ascending=False).reset_index(drop=True)
            depot_volume["Cum_Volume"] = depot_volume[VOLUME_COL].cumsum().round(0)
            depot_volume["Cum_Percentage"] = 100 * depot_volume["Cum_Volume"] / depot_volume[VOLUME_COL].sum().round(0)

            def classify(pct):
                if pct <= 70:
                    return "A"
                elif pct <= 90:
                    return "B"
                else:
                    return "C"

            depot_volume["ABC_Category"] = depot_volume["Cum_Percentage"].apply(classify)

            fig = go.Figure()
            fig.add_bar(
                x=depot_volume["DBF_DEPOT"],
                y=depot_volume[VOLUME_COL],
                name="Depot Volume",
                marker_color=depot_volume["ABC_Category"].map({"A": "green", "B": "orange", "C": "red"}),
                hovertemplate="<b>%{x}</b><br>Volume: %{y:,.0f}<extra></extra>"

            )
            fig.add_trace(
                go.Scatter(
                    x=depot_volume["DBF_DEPOT"],
                    y=depot_volume["Cum_Percentage"],
                    mode="lines+markers",
                    name="Cumulative %",
                    yaxis="y2",
                )
            )
            fig.update_layout(
                title="Depot-wise ABC Analysis",
                yaxis=dict(title="Volume"),
                yaxis2=dict(title="Cumulative %", overlaying="y", side="right", range=[0, 110]),
            )
            st.plotly_chart(fig, use_container_width=True)
            st.dataframe(depot_volume.round(0))
        else:
            st.info("DBF_DEPOT column not found.")
        st.markdown("""
### **Insights:**
- KALABURAGI, BIDAR, and VIJAYAPURA are top-tier, as they are the primary drivers of volume. 
- Orange depots (KOPPAL to CHITRADURGA) have the highest growth potential to secure the next wave of A-Class contributors. 
- Red depots have lowest contribution to the shipment volume - CHIKKODI AND GOKAK being the most risky depots.

""")

    # ---- Region Donut ----
    with tab4:
        st.markdown("###  Question: Which regions account for the largest share of shipments?")
        st.subheader("Region-wise Volume Share")
        if "DBF_REGION" in df_filtered.columns:
            region_volume = df_filtered.groupby("DBF_REGION")[VOLUME_COL].sum().round(0).reset_index()
            fig = px.pie(region_volume, values=VOLUME_COL, names="DBF_REGION", hole=0.5,
                         title="Volume Distribution by Region")
            fig.update_traces(
                texttemplate="%{label}: %{percent:.0%}",
                textinfo="label+percent",
                hovertemplate="<b>%{label}</b><br>Volume: %{value:,.0f}<br>Share: %{percent:.0%}<extra></extra>",
                insidetextorientation='auto'
            )

            st.plotly_chart(fig, use_container_width=True)
            st.dataframe(region_volume.set_index("DBF_REGION").round(0))
        else:
            st.info("DBF_REGION column not found.")
        st.markdown("""
### **Insights:**
- NORTH KARNATAKA 2 is the primary driver of volume contributing 60% to the shipments.
- Whereas NORTH KARNATAKA 1 is lagging by contributing the rest of 40%.
""")

    # ---- Region Stacked ----
    with tab5:
        st.markdown("###  Question: Which outlets contribute most to regional shipment volume?")
        st.subheader("Outlets & Volume by Region (100% Share)")
        if "DBF_REGION" in df_filtered.columns and "DBF_OUTLET_CODE" in df_filtered.columns:
            region_stats = df_filtered.groupby("DBF_REGION").agg(
                Outlet_Count=("DBF_OUTLET_CODE", "nunique"),
                Total_Volume=(VOLUME_COL, "sum"),
            ).reset_index()

            region_stats["Outlet %"] = (region_stats["Outlet_Count"] / region_stats["Outlet_Count"].sum().round(0)) * 100
            region_stats["Volume %"] = (region_stats["Total_Volume"] / region_stats["Total_Volume"].sum().round(0)) * 100

            melted = region_stats.melt(
                id_vars="DBF_REGION",
                value_vars=["Outlet %", "Volume %"],
                var_name="Metric",
                value_name="Percent",
            )

            fig = px.bar(melted, x="DBF_REGION", y="Percent", color="Metric",
                         barmode="stack", text=melted["Percent"].round(0),
                         title="Outlets vs Volume % by Region")
            fig.update_traces(texttemplate="%{text:.0f}%", textposition="inside")
            st.plotly_chart(fig, use_container_width=True)
            st.dataframe(region_stats.set_index("DBF_REGION").round(0))
        else:
            st.info("DBF_REGION or DBF_OUTLET_CODE not found.")
        with st.container():
            st.markdown("""
### **Insights:**
The chart depicts that 61% of Number of outlets in North Karnataka 1 contribute to just 40% of total shipment volume, whereas only 39% of Number of Outlets in North Karnataka 2 drive for 60% of total shipment volume, indicating that North Karnataka 2 region is the core volume driver

""")

    # ---- Special Outlets ----
    with tab6:
        st.markdown("###  Question: How does shipment performance differ between Hubbali and Belagavi?")
        st.subheader("Focused Analysis: Hubbali & Belagavi Depots")

        view_mode = st.radio("View Mode", ["Absolute", "Percentage"], horizontal=True, key="special_view_mode")
        depot_group_choice = st.radio("Select Depot Group", ["All", "Hubbali", "Belagavi"], horizontal=True, key="special_depot_group")

        patterns = {
            "Hubbali": ["HUBBALLI-1", "HUBBALLI-2"],
            "Belagavi": ["BELAGAVI", "BELAGAVI-2"],
            "All": ["HUBBALLI-1", "HUBBALLI-2", "BELAGAVI", "BELAGAVI-2"]
        }

        selected_patterns = patterns[depot_group_choice]

        df_special = df_filtered[
            df_filtered["DBF_DEPOT"].str.upper().str.contains("|".join([p.upper() for p in selected_patterns]), na=False)
        ]

        if not df_special.empty:
            comp = df_special.groupby("DBF_DEPOT")[VOLUME_COL].sum().round(0).reset_index()

            if view_mode == "Percentage":
                comp["Value"] = (comp[VOLUME_COL] / comp[VOLUME_COL].sum().round(0)) * 100
                value_col = "Value"
                title_suffix = " (%)"
            else:
                comp["Value"] = comp[VOLUME_COL]
                value_col = "Value"
                title_suffix = ""

            fig_pie = px.pie(comp, values=value_col, names="DBF_DEPOT",
                            title=f"Volume Distribution (Special Depots){title_suffix}", hole=0.4)
            fig_pie.update_traces(
                texttemplate="%{label}: %{percent:.0%}",        # inside slice (rounded percentage)
                hovertemplate="<b>%{label}</b><br>Volume: %{value:,.0f}<br>Share: %{percent:.0%}<extra></extra>",  # hover tooltip
                insidetextorientation='auto'              # prevents awkward rotated text
            )
            
            st.plotly_chart(fig_pie, use_container_width=True)

            outlet_counts = df_special.groupby("DBF_DEPOT")["DBF_OUTLET_CODE"].nunique().reset_index()
            outlet_counts.rename(columns={"DBF_OUTLET_CODE": "Unique_Outlet_Count"}, inplace=True)

            fig_bar = px.bar(outlet_counts, x="DBF_DEPOT", y="Unique_Outlet_Count", text="Unique_Outlet_Count",
                            title="Number of Unique Outlets (Special Depots)")
            fig_bar.update_traces(textposition="outside")
            st.plotly_chart(fig_bar, use_container_width=True)

            trend = df_special.groupby(["YearMonth", "DBF_DEPOT"])[VOLUME_COL].sum().reset_index()
            trend["YearMonth"] = trend["YearMonth"].astype(str)

            if view_mode == "Percentage":
                trend["Total"] = trend.groupby("YearMonth")[VOLUME_COL].transform("sum")
                trend["Value"] = ((trend[VOLUME_COL] / trend["Total"]) * 100).round(0)
                y_title = "Volume Share (%)"
            else:
                trend["Value"] = trend[VOLUME_COL]
                y_title = "Volume"

            fig_trend = px.line(trend, x="YearMonth", y="Value", color="DBF_DEPOT",
                                title=f"Monthly Shipment Trend (Special Depots){title_suffix}", markers=True)
            fig_trend.update_yaxes(title_text=y_title)
            st.plotly_chart(fig_trend, use_container_width=True)
            st.markdown("""
### **Insights:**
All four entities, particularly the top two (BELAGAVI and HUBBALLI-1), contribute a significant portion of the volume - 35% and 30% respectively.
BELAGAVI 2 AND HUBALLI 2 are contributing fairly lesser - 17% and 18% respectively.
""")


        # ---- TAB 7: Depot Map View ----
    with tab7:
        st.markdown("### 🚛 Question: What is the geographic spread of shipment volumes?")
        st.subheader("📍 Advanced Depot Shipment Volume Map")
    
        # --- Approximate and refined coordinates for Karnataka depots ---
        DEPOT_COORDS = {
            "BIDAR": (17.9133, 77.5301),
            "KALABURAGI": (17.3297, 76.8343),
            "SEDAM": (17.1794, 77.2830),
            "YADHAGIRI": (16.7666, 77.1376),
            "RAICHUR": (16.2046, 77.3555),
            "SINDHANUR": (15.7689, 76.7557),
            "BALLARI": (15.1394, 76.9214),
            "HOSAPETE": (15.2843, 76.3771),
            "KOPPAL": (15.3452, 76.1545),
            "BAGALKOT": (16.1867, 75.6962),
            "JAMAKHANDI": (16.5048, 75.2911),
            "VIJAYAPURA": (16.8302, 75.7100),
            "BELAGAVI": (15.8497, 74.4977),
            "BELAGAVI-2": (15.8497, 74.4977),
            "CHIKODI": (16.4295, 74.6005),
            "GOKAK": (16.1691, 74.8231),
            "HUBBALLI-1": (15.3647, 75.1239),
            "HUBBALLI-2": (15.3647, 75.1239),
            "GADAG": (15.4310, 75.6297),
            "HAVERI": (14.7951, 75.3979),
            "DAVANGERE": (14.4663, 75.9238),
            "CHITRADURGA": (14.2290, 76.3980),
        }
    
        if "DBF_DEPOT" in df_filtered.columns:
    # --- Aggregate shipment volume ---
            depot_volume_map = df_filtered.groupby("DBF_DEPOT")[VOLUME_COL].sum().reset_index()
            depot_volume_map["Latitude"] = depot_volume_map["DBF_DEPOT"].map(lambda x: DEPOT_COORDS.get(x, (None, None))[0])
            depot_volume_map["Longitude"] = depot_volume_map["DBF_DEPOT"].map(lambda x: DEPOT_COORDS.get(x, (None, None))[1])
            depot_volume_map = depot_volume_map.dropna(subset=["Latitude", "Longitude"])
        
            # --- Map center ---
            center_lat = depot_volume_map["Latitude"].mean()
            center_lon = depot_volume_map["Longitude"].mean()
        
                       # --- Hover text ---
            depot_volume_map["HoverText"] = depot_volume_map.apply(
                lambda row: f"<b>Location:</b> {row['DBF_DEPOT']}<br><b>Volume:</b> {row[VOLUME_COL]:,.0f}", axis=1
            )
            
            # --- Plot advanced map ---
            fig = px.scatter_mapbox(
                depot_volume_map,
                lat="Latitude",
                lon="Longitude",
                size=VOLUME_COL,
                color=VOLUME_COL,
                hover_name=None,  # remove default hover_name
                hover_data=None,  # remove other hover data
                color_continuous_scale="Viridis",
                size_max=55,
                zoom=6,
                mapbox_style="carto-positron",
            )
            
            # --- Styling ---
            fig.update_traces(
                marker=dict(
                    opacity=0.85,
                    sizemode="area",
                    sizeref=2.0 * max(depot_volume_map[VOLUME_COL]) / (55**2),
                ),
                hovertemplate="%{customdata}<extra></extra>",  # use customdata for hover
                customdata=depot_volume_map["HoverText"]
            )

        
            # --- Layout polish ---
            fig.update_layout(
                mapbox_center={"lat": center_lat, "lon": center_lon},
                coloraxis_colorbar=dict(
                    title="Shipment<br>Volume",
                    thickness=15,
                    len=0.75,
                ),
                margin=dict(l=0, r=0, t=60, b=0),
                font=dict(size=13),
                paper_bgcolor="rgba(0,0,0,0)",
                plot_bgcolor="rgba(0,0,0,0)",
                title="Depot Shipment Volume Heat Map (Bubble Chart View)",
            )
        
            st.plotly_chart(fig, use_container_width=True)

            # --- Data table ---
            #st.markdown("#### 📊 Depot-wise Shipment Summary")
            #st.dataframe(
                #depot_volume_map[["DBF_DEPOT", VOLUME_COL, "Latitude", "Longitude"]]
                #.set_index("DBF_DEPOT")
                #.sort_values(VOLUME_COL, ascending=False)
                #.round(0)
            #)
    
    with tab8:
        st.markdown("### Question: How has depot volume changed YoY?")
        st.subheader("Depot-wise YoY Volume Change")
    
        if "DBF_DEPOT" in df_filtered.columns and "ACTUAL_DATE" in df_filtered.columns:
            # Ensure ACTUAL_DATE is datetime
            df_filtered["ACTUAL_DATE"] = pd.to_datetime(df_filtered["ACTUAL_DATE"], errors="coerce")
            df_filtered["Year"] = df_filtered["ACTUAL_DATE"].dt.year
    
            # Aggregate volume by depot and year
            depot_yearly = df_filtered.groupby(["DBF_DEPOT", "Year"])["VOLUME"].sum().reset_index()
    
            # Pivot to have years as columns
            pivot_df = depot_yearly.pivot(index="DBF_DEPOT", columns="Year", values="VOLUME").fillna(0)
    
            # Only consider 2023 and 2024
            if 2023 in pivot_df.columns and 2024 in pivot_df.columns:
                pivot_df["YoY_Change"] = pivot_df[2024] - pivot_df[2023]
                pivot_df["YoY_Percentage"] = ((pivot_df[2024] - pivot_df[2023]) / pivot_df[2023]) * 100
                pivot_df["YoY_Percentage"] = pivot_df["YoY_Percentage"].round(2)
    
                # Sort depots by YoY change for waterfall
                pivot_df = pivot_df.sort_values("YoY_Change", ascending=False)
    
                # Waterfall chart with hover showing absolute and % change
                fig = go.Figure(go.Waterfall(
                    name="YoY Change",
                    orientation="v",
                    measure=["relative"] * len(pivot_df),
                    x=pivot_df.index,
                    y=pivot_df["YoY_Change"],
                    text=pivot_df["YoY_Change"].apply(lambda x: f"{x:,.0f}"),
                    textposition="outside",
                    connector={"line": {"color": "rgb(63, 63, 63)"}},
                    hovertemplate="<b>%{x}</b><br>Volume Change: %{y:,.0f}<br>Percentage Change: %{customdata:.2f}%<extra></extra>",
                    customdata=pivot_df["YoY_Percentage"]
                ))
    
                fig.update_layout(
                    title="Depot-wise YoY Volume Change (2023 → 2024)",
                    yaxis=dict(title="Volume Change"),
                    height=600,
                    margin=dict(b=150),
                )
    
                st.plotly_chart(fig, use_container_width=True)
    
                # Add percentage change to summary table
                summary_df = pivot_df[[2023, 2024, "YoY_Change", "YoY_Percentage"]].round(0)
                st.dataframe(summary_df)
    
                # -------------------------------------------------------------------------
                # NEW SECTION: YoY Change in Disposable Income (Per Capita Income)
                # -------------------------------------------------------------------------
                st.markdown("### YoY Change in Disposable Income (Per Capita Income)")
    
                # Load PCI data
                df_pci = pd.read_excel(
                    "https://docs.google.com/spreadsheets/d/1Pg0DkCaqQJymbrkIIqAcjbgCa-7MVHJB/export?format=xlsx",
                    sheet_name="PCI"
                )
                df_pci.columns = [c.strip() for c in df_pci.columns]
                df_pci.rename(columns={"Row Labels": "Location"}, inplace=True)
                df_pci["Location"] = df_pci["Location"].str.strip().str.upper()
    
                
    
                # Calculate YoY change in PCI
                if "Per capita - 2022-23" in df_pci.columns and "per capita - 2023-24" in df_pci.columns:
                    df_pci["PCI_YoY_Change"] = df_pci["per capita - 2023-24"] - df_pci["Per capita - 2022-23"]
                    df_pci["PCI_YoY_%Change"] = (
                        df_pci["PCI_YoY_Change"] / df_pci["Per capita - 2022-23"]
                    ) * 100
                    df_pci["PCI_YoY_%Change"] = df_pci["PCI_YoY_%Change"].round(2)
    
                    df_pci_sorted = df_pci.sort_values("PCI_YoY_%Change", ascending=False)
    
                    # Bar chart for PCI YoY change
                    fig_pci = px.bar(
                        df_pci_sorted,
                        x="PCI_YoY_%Change",
                        y="Location",
                        orientation="h",
                        color="PCI_YoY_%Change",
                        color_continuous_scale="Tealgrn",
                        text=df_pci_sorted["PCI_YoY_%Change"].apply(lambda x: f"{x:.1f}%")
                    )
    
                    fig_pci.update_layout(
                        title="YoY % Change in Per Capita Income (2023 → 2024)",
                        xaxis_title="YoY % Change in PCI",
                        yaxis_title="Location",
                        height=600,
                        template="plotly_dark",
                        margin=dict(t=60, b=80)
                    )
    
                    fig_pci.update_traces(textposition="outside")
                    st.plotly_chart(fig_pci, use_container_width=True)
    
                    # Display PCI summary table
                    st.dataframe(df_pci_sorted[["Location", "Per capita - 2022-23", "per capita - 2023-24", "PCI_YoY_%Change"]])
                else:
                    st.warning("PCI columns for 2022-23 or 2023-24 are missing in the dataset.")
    
            else:
                st.info("Data for 2023 and/or 2024 is missing.")
        else:
            st.info("DBF_DEPOT or ACTUAL_DATE column not found.")

# ===============================
# Entry Point
# ===============================
if __name__ == "__main__":
    run()
